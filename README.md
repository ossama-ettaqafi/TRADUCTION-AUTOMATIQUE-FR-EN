# ğŸŒ Traduction Automatique (FranÃ§ais-Anglais)

Un systÃ¨me complet de traduction automatique neuronale utilisant des modÃ¨les MarianMT fine-tunÃ©s sur le corpus Europarl.

## ğŸ“‹ Table des MatiÃ¨res

- [AperÃ§u du Projet](#-aperÃ§u-du-projet)
- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Utilisation Rapide](#-utilisation-rapide)
- [Guide DÃ©taillÃ©](#-guide-dÃ©taillÃ©)
- [RÃ©sultats](#-rÃ©sultats)
- [API](#-api)
- [DÃ©pannage](#-dÃ©pannage)

## ğŸ¯ AperÃ§u du Projet

Ce projet implÃ©mente un pipeline complet pour la traduction automatique bidirectionnelle franÃ§ais-anglais. Le systÃ¨me utilise l'architecture MarianMT de Hugging Face, fine-tunÃ©e sur le corpus Europarl pour des performances optimisÃ©es.

### Technologies UtilisÃ©es

- **ğŸ¤— Transformers** : ModÃ¨les MarianMT prÃ©-entraÃ®nÃ©s
- **âš¡ PyTorch** : Backend d'entraÃ®nement et d'infÃ©rence
- **ğŸ”¤ SentencePiece** : Tokenisation sous-mots
- **ğŸ“Š Europarl** : Corpus parallÃ¨le de qualitÃ©
- **ğŸŒ Flask** : Interface web

## âœ¨ FonctionnalitÃ©s

### ğŸ”„ Traduction Bidirectionnelle
- **FranÃ§ais â†’ Anglais**
- **Anglais â†’ FranÃ§ais**
- Support d'autres paires de langues (extensible)

### ğŸ› ï¸ Pipeline Complet
- **Nettoyage** et alignement des donnÃ©es
- **Tokenisation** avancÃ©e avec SentencePiece
- **EntraÃ®nement** avec fine-tuning
- **Ã‰valuation** automatique avec mÃ©trique BLEU
- **DÃ©ploiement** via interface console et web

### ğŸ“ˆ Ã‰valuation DÃ©taillÃ©e
- Score BLEU et prÃ©cisions n-grammes
- Analyse qualitative des erreurs
- Comparaison cible/prÃ©diction
- MÃ©triques par phrase

## ğŸ—ï¸ Architecture

```
traduction-automatique/
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ ğŸ“ raw/                 # DonnÃ©es brutes Europarl
â”‚   â”œâ”€â”€ ğŸ“ processed/           # DonnÃ©es nettoyÃ©es
â”‚   â””â”€â”€ ğŸ“ embeddings/          # Encodages numÃ©riques
â”œâ”€â”€ ğŸ“ models/
â”‚   â””â”€â”€ ğŸ“ marianmt/
â”‚       â”œâ”€â”€ ğŸ“ en-fr/          # ModÃ¨le Anglaisâ†’FranÃ§ais
â”‚       â””â”€â”€ ğŸ“ fr-en/          # ModÃ¨le FranÃ§aisâ†’Anglais
â”œâ”€â”€ ğŸ“ web/                    # Application Flask
â”œâ”€â”€ ğŸ”§ explore.py              # Analyse des donnÃ©es
â”œâ”€â”€ ğŸ§¹ preprocessing.py        # PrÃ©traitement
â”œâ”€â”€ ğŸ‹ï¸â€â™‚ï¸ train.py               # EntraÃ®nement
â”œâ”€â”€ ğŸ“Š evaluate_bleu.py        # Ã‰valuation
â”œâ”€â”€ ğŸ’» app.py                  # Interface console
â””â”€â”€ ğŸ“– README.md
```

## âš™ï¸ Installation

### PrÃ©requis SystÃ¨me

- Python 3.8+
- 8GB+ RAM (16GB recommandÃ©)
- GPU NVIDIA (optionnel mais recommandÃ© pour l'entraÃ®nement)

### Installation des DÃ©pendances

```bash
# Cloner le repository
git clone <votre-repo>
cd traduction-automatique

# CrÃ©er un environnement virtuel (optionnel)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Installer les dÃ©pendances
pip install torch transformers datasets sentencepiece tqdm evaluate flask

# VÃ©rifier l'installation
python -c "import torch; print('PyTorch:', torch.__version__)"
```

### TÃ©lÃ©chargement des DonnÃ©es

1. **TÃ©lÃ©charger Europarl v7** :
```bash
# TÃ©lÃ©charger depuis https://www.statmt.org/europarl/
mkdir -p data/raw
# Placer les fichiers dans data/raw/
# - europarl-v7.fr-en.en
# - europarl-v7.fr-en.fr
```

## ğŸš€ Utilisation Rapide

### Mode Express (ModÃ¨les PrÃ©-entraÃ®nÃ©s)

```bash
# Utiliser directement les modÃ¨les Hugging Face
python app.py
```

### Pipeline Complet (RecommandÃ©)

```bash
# 1. Analyse des donnÃ©es
python explore.py

# 2. PrÃ©traitement (30-60 minutes)
python preprocessing.py

# 3. EntraÃ®nement (2-4 heures avec GPU)
python train.py

# 4. Ã‰valuation
python evaluate_bleu.py

# 5. Utilisation
python app.py
```

## ğŸ“š Guide DÃ©taillÃ©

### 1. Exploration des DonnÃ©es

```bash
python explore.py
```
**Sortie attendue :**
```
[ğŸ“„] Fichier brut : europarl-v7.fr-en.en
   Nombre de phrases : 2,000,000
   Nombre total de mots : 45,000,000
   Longueur moyenne : 22.5 mots
```

### 2. PrÃ©traitement AvancÃ©

Le script `preprocessing.py` effectue :

- **Nettoyage** : Normalisation, suppression HTML, filtrage
- **Alignement** : Paires parallÃ¨les cohÃ©rentes
- **Tokenisation** : SentencePiece avec vocabulaire 16k
- **Encodage** : Conversion en IDs numÃ©riques

### 3. EntraÃ®nement des ModÃ¨les

**Configuration par dÃ©faut :**
```python
BATCH_SIZE = 8
EPOCHS = 2
MAX_LENGTH = 128
MAX_SAMPLES = 500  # Ajuster selon vos ressources
```

**Pour un entraÃ®nement complet :**
```python
MAX_SAMPLES = 500000  # 500k Ã©chantillons
EPOCHS = 3
BATCH_SIZE = 16  # Si GPU avec >8GB VRAM
```

### 4. Ã‰valuation des Performances

```bash
python evaluate_bleu.py
```

**MÃ©triques fournies :**
- Score BLEU (1-4 grammes)
- PrÃ©cisions individuelles
- PÃ©nalitÃ© de briÃ¨vetÃ©
- Top 5 des erreurs

## ğŸ“Š RÃ©sultats

### Performances Typiques

| ModÃ¨le | BLEU Score | PrÃ©cision 1-gram | Temps d'EntraÃ®nement |
|--------|------------|------------------|---------------------|
| Base (Helsinki-NLP) | ~35.2 | ~0.55 | - |
| Fine-tunÃ© (500 Ã©ch.) | ~32.4 | ~0.52 | 30 min |
| Fine-tunÃ© (50k Ã©ch.) | ~38.1 | ~0.61 | 4 heures |

### Exemple de Traduction

**EntrÃ©e (EN) :** "The committee will examine the proposal next week."
**Sortie (FR) :** "La commission examinera la proposition la semaine prochaine."

## ğŸŒ API et Interfaces

### Interface Console

```bash
python app.py

# Sortie :
Choisir la direction ('en->fr' ou 'fr->en') : en->fr
Texte Ã  traduire : Hello, how are you?
Traduction â†’ Bonjour, comment allez-vous ?
```

### Application Web Flask

```bash
cd web
python app.py
# AccÃ©der Ã  http://localhost:5000
```

### API REST

```python
import requests

response = requests.post(
    "http://localhost:5000/translate",
    json={
        "text": "This is a test sentence.",
        "direction": "en->fr"
    }
)
print(response.json()["translation"])
```

## ğŸ”§ Configuration AvancÃ©e

### HyperparamÃ¨tres d'EntraÃ®nement

Modifier dans `train.py` :

```python
training_args = Seq2SeqTrainingArguments(
    output_dir=str(output_dir),
    num_train_epochs=4,           # Plus d'Ã©poques
    per_device_train_batch_size=16, # Batch plus grand
    learning_rate=5e-5,           # Taux d'apprentissage
    warmup_steps=500,             # Warmup
    weight_decay=0.01,            # RÃ©gularisation
    fp16=True,                    # Acceleration GPU
)
```

### Tokenisation PersonnalisÃ©e

Dans `preprocessing.py` :

```python
# Pour d'autres langues
VOCAB_SIZE = 32000
MODEL_TYPE = "bpe"  # "unigram" ou "bpe"
CHARACTER_COVERAGE = 1.0  # Pour couvrir tous les caractÃ¨res
```

## ğŸ› DÃ©pannage

### ProblÃ¨mes Courants

**Erreur MÃ©moire Insuffisante**
```python
# Solution : RÃ©duire la configuration
BATCH_SIZE = 4
MAX_LENGTH = 64
MAX_SAMPLES = 100
```

**Fichiers Manquants**
```bash
# VÃ©rifier la structure
ls data/raw/
# Devrait afficher : europarl-v7.fr-en.en et europarl-v7.fr-en.fr
```

**EntraÃ®nement Trop Lent**
- Activer CUDA : `torch.cuda.is_available()`
- Utiliser FP16 : `fp16=True`
- RÃ©duire `MAX_SAMPLES`

### Extensions Possibles

1. **Nouvelles Paires de Langues**
```python
# Ajouter dans train.py
MODEL_CONFIGS = {
    "en-de": "Helsinki-NLP/opus-mt-en-de",
    "en-es": "Helsinki-NLP/opus-mt-en-es",
    # ...
}
```

2. **Interface Graphique**
- Streamlit pour le prototypage
- Gradio pour dÃ©monstrations
- Interface React avancÃ©e

3. **DÃ©ploiement Production**
- Container Docker
- API FastAPI
- Scaling avec multiples GPUs

## ğŸ¤ Contribution

Les contributions sont bienvenues ! Voici comment participer :

1. **Signaler un bug** : Ouvrir une issue avec les Ã©tapes pour reproduire
2. **SuggÃ©rer une amÃ©lioration** : Proposer de nouvelles fonctionnalitÃ©s
3. **Soumettre du code** : Pull request avec tests et documentation

### DÃ©veloppement

```bash
# Setup dÃ©veloppement
git clone <repo>
cd traduction-automatique
pip install -e .[dev]

# Lancer les tests
python -m pytest tests/

# VÃ©rifier le style de code
flake8 scripts/
```

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

**Note importante** : Les modÃ¨les prÃ©-entraÃ®nÃ©s Helsinki-NLP sont sous leur propre licence. Consultez les conditions d'utilisation sur [Hugging Face](https://huggingface.co/Helsinki-NLP).

## ğŸ™ Remerciements

- **Hugging Face** pour l'excellente bibliothÃ¨que Transformers
- **Union EuropÃ©enne** pour le corpus Europarl
- **CommunautÃ© Open Source** pour les outils et ressources

*âœ¨ Fait avec passion pour l'apprentissage automatique et la linguistique computationnelle âœ¨*
