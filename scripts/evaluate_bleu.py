# ============================================================
# üì¶ IMPORTS
# ============================================================
import torch
from transformers import MarianMTModel, MarianTokenizer
from tqdm import tqdm
import evaluate
from pathlib import Path
import warnings

warnings.filterwarnings("ignore")

# ============================================================
# ‚öôÔ∏è CONFIGURATION
# ============================================================
MODEL_DIR = Path("models/marianmt/en-fr")   # Mod√®le MarianMT fine-tun√©
DATA_SRC = Path("data/processed/europarl_tok.en")  # Corpus source
DATA_REF = Path("data/processed/europarl_tok.fr")  # Traductions de r√©f√©rence
MAX_SAMPLES = 200                            # Nombre de phrases √† √©valuer
MAX_LENGTH = 128
BATCH_SIZE = 8
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Chargement de la m√©trique BLEU
bleu_metric = evaluate.load("bleu")

# ============================================================
# üß† FONCTIONS PRINCIPALES
# ============================================================

def load_data(max_samples):
    """Charge les phrases source et de r√©f√©rence (align√©es ligne √† ligne)."""
    with DATA_SRC.open(encoding="utf-8") as f_src, DATA_REF.open(encoding="utf-8") as f_ref:
        src_sentences = [line.strip() for _, line in zip(range(max_samples), f_src)]
        ref_sentences = [line.strip() for _, line in zip(range(max_samples), f_ref)]
    return src_sentences, ref_sentences


def translate_and_analyze(model_dir, src_sentences, ref_sentences):
    """
    Traduit un √©chantillon de phrases avec MarianMT,
    calcule le score BLEU global et analyse les erreurs.
    """
    print(f"\n[INFO] Chargement du mod√®le depuis {model_dir} ({DEVICE})...")
    tokenizer = MarianTokenizer.from_pretrained(model_dir)
    model = MarianMTModel.from_pretrained(model_dir).to(DEVICE)
    model.eval()

    translations = []

    print(f"[INFO] G√©n√©ration de {len(src_sentences)} traductions...")
    for i in tqdm(range(0, len(src_sentences), BATCH_SIZE), desc="Traduction en cours"):
        batch = src_sentences[i:i+BATCH_SIZE]
        inputs = tokenizer(batch, return_tensors="pt", truncation=True, padding=True, max_length=MAX_LENGTH).to(DEVICE)

        with torch.no_grad():
            outputs = model.generate(**inputs, max_length=MAX_LENGTH)

        decoded = [tokenizer.decode(t, skip_special_tokens=True) for t in outputs]
        translations.extend(decoded)

    # ===== √âVALUATION BLEU =====
    global_references = [[r] for r in ref_sentences]
    results = bleu_metric.compute(predictions=translations, references=global_references)

    # ===== ANALYSE D‚ÄôERREURS =====
    error_metrics = []
    for i, (pred, ref) in enumerate(zip(translations, ref_sentences)):
        # Calcul de la pr√©cision unigramme (BLEU-1)
        result = bleu_metric.compute(predictions=[pred], references=[[ref]], max_order=1)
        unigram_precision = result['precisions'][0]

        error_metrics.append({
            'unigram_precision': unigram_precision,
            'pred': pred,
            'ref': ref,
            'src': src_sentences[i]
        })

    # Tri : du plus faible score unigramme (erreurs les plus graves)
    sorted_errors = sorted(error_metrics, key=lambda x: x['unigram_precision'])
    return results, translations, sorted_errors


# ============================================================
# üßæ RAPPORT FINAL
# ============================================================

def main():
    print("\nüåç √âVALUATION DU MOD√àLE MARIANMT (EN‚ÜíFR)")
    print("="*60)

    # 1Ô∏è‚É£ Charger les donn√©es
    src_sentences, ref_sentences = load_data(MAX_SAMPLES)

    # 2Ô∏è‚É£ Traduire et √©valuer
    results, translations, sorted_errors = translate_and_analyze(
        MODEL_DIR, src_sentences, ref_sentences
    )

    # 3Ô∏è‚É£ Afficher le rapport global
    print("\n===== RAPPORT D'√âVALUATION =====")
    print(f"Nombre de phrases √©valu√©es : {len(translations)}")
    print(f"Score BLEU global : {results['bleu'] * 100:.2f}")
    print(f"Pr√©cisions n-grammes (P1, P2, P3, P4) : {[round(p,3) for p in results['precisions']]}")
    print(f"P√©nalit√© de bri√®vet√© : {results['brevity_penalty']:.4f}")

    # 4Ô∏è‚É£ Exemples d'erreurs fr√©quentes
    print("\n===== TOP 5 DES PIRES TRADUCTIONS =====")
    for i, err in enumerate(sorted_errors[:5]):
        print(f"\n--- Erreur #{i+1} (Pr√©cision unigramme : {err['unigram_precision']:.3f}) ---")
        print(f"[EN]  {err['src']}")
        print(f"[FR REF]  {err['ref']}")
        print(f"[FR GEN]  {err['pred']}")

    print("\n[FIN] √âvaluation termin√©e avec succ√®s.")
    print("="*60)


# ============================================================
# üöÄ LANCEMENT DU SCRIPT
# ============================================================
if __name__ == "__main__":
    main()
