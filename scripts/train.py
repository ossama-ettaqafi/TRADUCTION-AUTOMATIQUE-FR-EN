# ============================================================
# üì¶ IMPORTS
# ============================================================
import os
import multiprocessing as mp
from pathlib import Path
import torch
from transformers import (
    MarianTokenizer,
    MarianMTModel,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments
)
import datasets  # Hugging Face Datasets

# ============================================================
# ‚öôÔ∏è FIX : compatibilit√© Windows + Python 3.12
# ============================================================
mp.set_start_method("spawn", force=True)
os.environ["PYTORCH_NO_MEM_TRACKING"] = "1"  # √©vite un bug d‚Äôallocation m√©moire

# ============================================================
# üîß CONFIGURATION G√âN√âRALE
# ============================================================
MODEL_NAME_EN_FR = "Helsinki-NLP/opus-mt-en-fr"  # mod√®le pr√©-entra√Æn√© EN‚ÜíFR
MODEL_NAME_FR_EN = "Helsinki-NLP/opus-mt-fr-en"  # mod√®le pr√©-entra√Æn√© FR‚ÜíEN

# Chemins des donn√©es
DATA_EN = Path("data/processed/europarl_tok.en")
DATA_FR = Path("data/processed/europarl_tok.fr")

# R√©pertoire de sortie pour sauvegarder les mod√®les fine-tun√©s
OUTPUT_DIR = Path("models/marianmt")

# Hyperparam√®tres
MAX_SAMPLES = 500     # Nombre d‚Äôexemples √† charger (pour test rapide)
MAX_LENGTH = 128      # Longueur maximale des phrases (troncature)
BATCH_SIZE = 8
EPOCHS = 2
TEST_SIZE = 0.1       # 10 % des donn√©es pour la validation


# ============================================================
# üìö 1Ô∏è‚É£ CHARGEMENT DES DONN√âES
# ============================================================
def load_parallel_data(en_path, fr_path, max_samples=None):
    """
    Charge un corpus parall√®le (anglais ‚Üî fran√ßais) ligne par ligne.
    Chaque ligne du fichier .en correspond √† la m√™me ligne dans .fr.
    """
    if not en_path.exists() or not fr_path.exists():
        raise FileNotFoundError("Les fichiers de donn√©es anglais/fran√ßais sont introuvables.")

    # Lecture limit√©e √† `max_samples` lignes
    with en_path.open(encoding="utf-8") as f_en, fr_path.open(encoding="utf-8") as f_fr:
        en_sentences = [line.strip() for _, line in zip(range(max_samples), f_en)]
        fr_sentences = [line.strip() for _, line in zip(range(max_samples), f_fr)]

    print(f"[INFO] {len(en_sentences)} paires de phrases charg√©es.")
    return en_sentences, fr_sentences


# ============================================================
# üß† 2Ô∏è‚É£ PR√âTRAITEMENT POUR LE TRAINER
# ============================================================
def preprocess_batch(batch, tokenizer, src_key, tgt_key, max_length=128):
    """
    Tokenise les phrases sources et cibles.
    Cr√©e des paires (input_ids, labels) compatibles avec Seq2SeqTrainer.
    """
    # Tokenisation des phrases source et cible
    inputs = tokenizer(
        [x[src_key] for x in batch["translation"]],
        truncation=True,
        padding="max_length",
        max_length=max_length
    )
    targets = tokenizer(
        [x[tgt_key] for x in batch["translation"]],
        truncation=True,
        padding="max_length",
        max_length=max_length
    )

    # Le Trainer de HuggingFace s‚Äôattend √† un champ "labels"
    inputs["labels"] = targets["input_ids"]
    return inputs


# ============================================================
# üèãÔ∏è‚Äç‚ôÇÔ∏è 3Ô∏è‚É£ ENTRA√éNEMENT DU MOD√àLE
# ============================================================
def train_model(src_lang, tgt_lang, model_name, output_subdir):
    """
    Entra√Æne un mod√®le MarianMT (EN‚ÜîFR ou FR‚ÜîEN) sur un petit corpus parall√®le.
    Sauvegarde le mod√®le fine-tun√© et le tokenizer.
    """
    print(f"\n[üöÄ] Entra√Ænement du mod√®le {src_lang} ‚Üí {tgt_lang}...")

    # --- Chargement du corpus parall√®le
    en_sentences, fr_sentences = load_parallel_data(DATA_EN, DATA_FR, MAX_SAMPLES)

    # --- S√©lection de la direction (EN‚ÜíFR ou FR‚ÜíEN)
    if src_lang == "fr" and tgt_lang == "en":
        src_sentences, tgt_sentences = fr_sentences, en_sentences
    else:
        src_sentences, tgt_sentences = en_sentences, fr_sentences

    # --- Construction du dataset HuggingFace
    dataset = datasets.Dataset.from_dict({
        "translation": [{"src": s, "tgt": t} for s, t in zip(src_sentences, tgt_sentences)]
    })

    # --- D√©coupage train / validation
    split = dataset.train_test_split(test_size=TEST_SIZE)
    train_dataset = split["train"]
    val_dataset = split["test"]

    # --- Chargement du tokenizer et du mod√®le pr√©-entra√Æn√©
    tokenizer = MarianTokenizer.from_pretrained(model_name)
    model = MarianMTModel.from_pretrained(model_name)

    # --- Pr√©traitement des batches (tokenisation + labels)
    train_dataset = train_dataset.map(
        lambda batch: preprocess_batch(batch, tokenizer, "src", "tgt", MAX_LENGTH),
        batched=True,
        remove_columns=["translation"]
    )
    val_dataset = val_dataset.map(
        lambda batch: preprocess_batch(batch, tokenizer, "src", "tgt", MAX_LENGTH),
        batched=True,
        remove_columns=["translation"]
    )

    # --- Dossier de sortie
    output_dir = OUTPUT_DIR / output_subdir
    output_dir.mkdir(parents=True, exist_ok=True)

    # --- Configuration de l‚Äôentra√Ænement
    training_args = Seq2SeqTrainingArguments(
        output_dir=str(output_dir),
        num_train_epochs=EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        logging_dir='./logs',
        logging_steps=20,
        save_steps=200,
        predict_with_generate=True,  # G√©n√©ration automatique pour l‚Äô√©val
        fp16=torch.cuda.is_available(),  # FP16 si GPU dispo
        report_to="none",  # Pas de WandB ou TensorBoard
        dataloader_num_workers=0,  # Important sous Windows
    )

    # --- Entra√Æneur
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer
    )

    # --- Lancement de l'entra√Ænement
    trainer.train()

    # --- Sauvegarde du mod√®le et du tokenizer
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)

    print(f"[‚úÖ] Mod√®le {src_lang} ‚Üí {tgt_lang} sauvegard√© dans : {output_dir}")


# ============================================================
# üß© 4Ô∏è‚É£ MAIN : PIPELINE COMPLET
# ============================================================
if __name__ == "__main__":
    # Entra√Ænement EN ‚Üí FR
    train_model("en", "fr", MODEL_NAME_EN_FR, "en-fr")

    # Entra√Ænement FR ‚Üí EN
    train_model("fr", "en", MODEL_NAME_FR_EN, "fr-en")

    print("\nüéØ Fine-tuning termin√© pour les deux directions de traduction.")
